{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pacman in Automation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Authors**: Corey Craddock, Braden Hogan, Justin Rentie, Wesley Turner\n",
    "\n",
    "**Date Submitted**: 12/12/2017\n",
    "\n",
    "**Description**: Create a game of Pacman with controls to vary levels of Pacman and Ghost intelligence and mazes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our CS 440 Final Project, we chose to implement artificial intelligence algorithms that we learned in the class over the course of this semester to run the classic arcade game Pacman in full automation. We decided this as our final project for a few reasons.\n",
    "\n",
    "First, all four of use have deeply enjoyed learning artificial intelligence using games this semester, and we wanted our last project to reflect this. \n",
    "\n",
    "Second, we have already seen the AI algorithms, which are Iterative Deepening Search and Q Table Reinforcement Learning, implemented within a game setting. We were expecting implementing these algorithms for a new game to be both an obtainable goal, as well as a challenging one.\n",
    "\n",
    "Last, in our initial research for this project, we found that implementing artificial intelligence with Pacman was a common project for computer scientists, taking the form of a rite of a passage for anyone deeply interested in artificial intelligence.\n",
    "\n",
    "Over the course of the last few weeks, we have worked diligently to implement a game of Pacman with these algorithms. Below, we discuss our implementation for the framework of the project, the object used to save the state and players, the Iterative Deepening Search algorithm with Pacman and Ghosts, and the Reinforcement Learning algorithm with Pacman."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've outlined thoughts on design and decisions that we made in implementing each of these sections here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Framework - Corey Craddock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Waiting for english from Corey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gameboard - Braden Hogan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Waiting for english from Braden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative Deepening Search - Justin Rentie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was our intention in designing our Pacman project that we would implement two different kinds of Artificial Intelligence for PacMan. We wanted to implement Iterative-Deepening Search for the first type of Artificial Intelligence.\n",
    "\n",
    "We referenced a couple lecture notes from class in implementing IDS for Pacman. We used Lecture 05 Iterative Deepening and Other Uninformed Search Methods and 06 Python Implementation of Iterative Deepening as an example of how to implement basic IDS for searching through a tree of possible states. We also relied on examples from Assignment 02. Links to these references are found below.\n",
    "\n",
    "We used Iterative Deepening Search to search within a given depth limit from possible actions that can be taken from pacman at any given state. If pacman locates a dot within the given depth limit the path to that dot is returned. \n",
    "\n",
    "The only major change that needed to be implemented was in the case of approaching a dot near any ghosts. In this situation we added a function called `fearFactor()` which will return `run` from `depthLimitedSearch()` which then in turn calls a function which removes possible moves from Pacman that could lead him towards a ghost. This results in Pacman effectively \"running away\" from the ghosts. \n",
    "\n",
    "This will go on until Pacman finds a dot within the given depth limit that is not in the direction of a ghost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # PacMan \"blood hunter\" AI. Copied from Ghost.\n",
    "    # Helps Intelligent Move in finding the best direction to take to get to nearest dot\n",
    "    def depthLimitedSearch(self, board, ghosts, closestDots, actions, takeAction, depthLimit):\n",
    "        if PacMan.fearFactor(self, ghosts):\n",
    "            return 'run'\n",
    "\n",
    "        for dot in closestDots:\n",
    "            if self.location == dot.location:\n",
    "                return []\n",
    "\n",
    "        if depthLimit == 0:\n",
    "            return \"cutoff\"\n",
    "\n",
    "        cutOffOccurred = False\n",
    "        for action in PacMan.actions(self, board):\n",
    "            copyBoard = copy.deepcopy(board)\n",
    "            copySelf = copy.deepcopy(self)\n",
    "            takeAction(copySelf, copyBoard, action)\n",
    "            result = PacMan.depthLimitedSearch(copySelf, copyBoard, ghosts, closestDots, actions, takeAction,\n",
    "                                              depthLimit - 1)\n",
    "\n",
    "            if result is \"cutoff\":\n",
    "                cutOffOccurred = True\n",
    "            elif result is not \"failure\":\n",
    "                return action\n",
    "        if cutOffOccurred:\n",
    "            return \"cutoff\"\n",
    "        else:\n",
    "            return \"failure\"\n",
    "\n",
    "    # Causes the ghost to scan through the board, making the most intelligent shortest path decision\n",
    "    def intelligentMove(self, board, ghosts, maxDepth=4):\n",
    "        closestDots = PacMan.getClosestDots(self, board)\n",
    "\n",
    "        for dot in closestDots:\n",
    "            if self.location == dot.location:\n",
    "                return\n",
    "\n",
    "        for depth in range(maxDepth):\n",
    "            result = PacMan.depthLimitedSearch(self, board, ghosts, closestDots, PacMan.actions, PacMan.takeAction,\n",
    "                                              depth)\n",
    "            if result != \"cutoff\" and result != \"failure\" and result != 'run':\n",
    "                print(\"PacMan found intelligent move. Returning intelligentMove\")\n",
    "                return PacMan.takeAction(self, board, result)\n",
    "\n",
    "        if(result == 'run'):\n",
    "            PacMan.runFromGhost(self, board, ghosts,  PacMan.actions)\n",
    "        else:\n",
    "            PacMan.makeRandomMove(self, board)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PlayGame\n",
    "import PacMan as P\n",
    "import Ghost as G\n",
    "import Dot as d\n",
    "import GameBoard as Board\n",
    "import copy as copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Really basic state to start with\n",
    "state = [['=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '='],\n",
    "        ['|', ' ', ' ', ' ', ' ', 'G', ' ', ' ', ' ', ' ', '|'],\n",
    "        ['|', ' ', '=', '=', '=', ' ', '=', '=', '=', ' ', '|'],\n",
    "        ['|', ' ', '|', ' ', '|', ' ', '|', ' ', '|', ' ', '|'],\n",
    "        ['|', ' ', '=', '=', '=', ' ', '=', '=', '=', ' ', '|'],\n",
    "        ['|', '.', '.', '.', '.', '.', '.', '.', '.', 'P', '|'],\n",
    "        ['=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Board' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-cbbb584b4d5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Start game with above state and 2 ghosts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mboard\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBoard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGameBoard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# Create a Pacman object using this board\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPacMan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpacManSpawnPt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Board' is not defined"
     ]
    }
   ],
   "source": [
    "# Start game with above state and 2 ghosts\n",
    "board = Board.GameBoard(state)\n",
    "# Create a Pacman object using this board\n",
    "p = P.PacMan(board.pacManSpawnPt)\n",
    "\n",
    "ghostsAvailable = [G.Ghost(board.ghostSpawnPt)]\n",
    "intelligenceLevel = 3\n",
    "\n",
    "# Train Q for p\n",
    "Q = []\n",
    "# Trains Q table and prints each game\n",
    "Q, scores = p.trainQ(board, 30, 0.5, 0.7, ghostsAvailable, intelligenceLevel, True)\n",
    "print(scores)\n",
    "\n",
    "# Runs startGame without Pacman intelligence and printing\n",
    "#startGame(board, p, ghostsAvailable, Q, intelligenceLevel, False, True)\n",
    "\n",
    "# Runs startGame with Pacman intelligence and not printing\n",
    "#startGame(board, p, ghostsAvailable, Q, intelligenceLevel, True, True)\n",
    "\n",
    "# Runs startGame with Q table and printing\n",
    "startGame(board, p, ghostsAvailable, Q, intelligenceLevel, False, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning - Wesley Turner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the beginning, we knew we wanted to write a function to train a Q table for the states that Pacman would most likely move through in a game. We planned for this to be our most optimized form of a smart Pacman, as this intelligence made the most sense for our game.\n",
    "\n",
    "We utilized two different sources to implement this. The first was from a previous assignment in this course, Assignment 5. A link for this assignment page can be found below. This assignment had students implement training a Q table and testing it with the Towers of Hanoi puzzle. Although this was a good start, that puzzle is a one-person game, so our reinforcement would need to differ vastly. However, the overall structure and design for the trainQ function initially came from this assignment.\n",
    "\n",
    "Secondly, we took inspiration for the reinforcement for Pacman from the lecture notes titled “Reinforcement Learning for Two-Player Games,” which I have linked below. We knew we needed to augment the reinforcement for different actions that Pacman may take, and the implementation in this lecture was sufficient to start our brainstorming.\n",
    "\n",
    "Despite these sources aiding us with design, we were still running into issues with what actions needed reinforcement. These were the actions we knew we could reinforce:\n",
    "\n",
    " - Change to score\n",
    " - Pacman dies\n",
    " - Pacman wins or loses\n",
    " - Pacman eats a dot\n",
    "\n",
    "We tried a variety of combinations of these reinforcements. For the most part, these were blind tests: which combination of reinforcements would lead Pacman to have the best and fastest results? We ended up utilizing the implementation below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def trainQ(self, board, nRepetitions, learningRate, epsilonDecayFactor, ghostsAvailable, intelligenceLevel, verbose=False):\n",
    "        maxGames = nRepetitions\n",
    "        rho = learningRate\n",
    "        epsilonDecayRate = epsilonDecayFactor\n",
    "        epsilon = 1.0\n",
    "        Q = {}\n",
    "        scores = []\n",
    "\n",
    "        for nGames in range(maxGames):\n",
    "            if verbose:\n",
    "                print(\"Game:\", nGames, \"; Starting game.\")\n",
    "            epsilon *= epsilonDecayRate\n",
    "            step = 0\n",
    "            state = copy.deepcopy(board)\n",
    "            copySelf = copy.deepcopy(self)\n",
    "            ghosts = []\n",
    "            score = 0\n",
    "            done = False\n",
    "\n",
    "            while not done and not copySelf.gameOver(state):\n",
    "                dead = False\n",
    "                copyGhosts = copy.deepcopy(ghostsAvailable)\n",
    "                step += 1\n",
    "                copyState = copy.deepcopy(state)\n",
    "                move = PacMan.epsilonGreedy(copySelf, epsilon, Q, copyState)\n",
    "\n",
    "                _, ghosts, copySelf, stateNew, score, dead = PlayGame.runSingleTurn(step, ghosts, copyGhosts, intelligenceLevel, copySelf, copyState, score, dead, move)\n",
    "                # Full return: turn, ghosts, p, board, score, dead\n",
    "\n",
    "                #Initial value\n",
    "                if PacMan.boardMoveTuple(copySelf, state, move) not in Q:\n",
    "                    Q[PacMan.boardMoveTuple(copySelf, state, move)] = 0  # initial Q value for new state,move\n",
    "\n",
    "                if stateNew.dotsLeft < state.dotsLeft:\n",
    "                    #Pacman ate a dot. Medium positive reinforcement\n",
    "                    Q[PacMan.boardMoveTuple(copySelf, state, move)] += 3\n",
    "                else:\n",
    "                    #Pacman did not eat a dot. Small negative reinforcement\n",
    "                    Q[PacMan.boardMoveTuple(copySelf, state, move)] += -1\n",
    "                if dead:\n",
    "                    #Pacman lost a life. Large negative reinforcement\n",
    "                    Q[PacMan.boardMoveTuple(copySelf, state, move)] = -10\n",
    "\n",
    "                if step > 1:\n",
    "                    Q[PacMan.boardMoveTuple(copySelf, stateOld, moveOld)] += rho * (Q[PacMan.boardMoveTuple(copySelf, state, move)] - Q[PacMan.boardMoveTuple(copySelf, stateOld, moveOld)])\n",
    "\n",
    "                stateOld, moveOld, scoreOld = state, move, score\n",
    "                state = stateNew\n",
    "            if verbose:\n",
    "                if copySelf.lives > 0:\n",
    "                    print(\"Pacman Won!\")\n",
    "                else:\n",
    "                    print(\"Pacman Lost!\")\n",
    "            scores.append(score)\n",
    "        return Q, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This combination of reinforcements were tested to have the best results for Pacman, as it allows him to have some flexibility when choosing a state/move that is not necessarily taking the closest path to a dot (i.e. he needs to run away). We will be displaying the performance that we found in using trainQ and the Q table it creates in a section below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are all methods and files in our project. We've decided to show who authored which section to show that we all worked as evenly as possible in implementing this project.\n",
    " \n",
    " - Dot.py - All methods authored by Justin Rentie\n",
    " - GameBoard.py - All methods authored by Braden Hogan\n",
    " - Ghost.py\n",
    "     - init - Corey Craddock & Justin Rentie\n",
    "     - actions - Corey Craddock & Braden Hogan\n",
    "     - takeAction - Corey Craddock, Braden Hogan, & Justin Rentie\n",
    "     - randomMove - Corey Craddock\n",
    "     - takeActionShortestDistance - Wesley Turner\n",
    "     - depthLimitedSearch - Wesley Turner\n",
    "     - intelligentMove - Wesley Turner\n",
    " - Pacman.py\n",
    "     - init - Corey Craddock & Justin Rentie\n",
    "     - spawnPt - Corey Craddock\n",
    "     - actions - Corey Craddock & Braden Hogan\n",
    "     - takeAction - Corey Craddock, Braden Hogan, & Justin Rentie\n",
    "     - calculateDistance - Justin Rentie\n",
    "     - getClosestDot - Justin Rentie\n",
    "     - directionToObj - Justin Rentie\n",
    "     - fearFactor - Justin Rentie\n",
    "     - depthLimitedSearch - Justin Rentie\n",
    "     - intelligentMove - Justin Rentie\n",
    "     - runFromGhost - Justin Rentie\n",
    "     - makeRandomMove - Justin Rentie\n",
    "     - gameOver - Corey Craddock\n",
    "     - getLives - Corey Craddock\n",
    "     - boardMoveTuple - Wesley Turner\n",
    "     - useReinforcementTable - Wesley Turner\n",
    "     - epsilonGreedy - Wesley Turner\n",
    "     - trainQ - Wesley Turner\n",
    " - PlayGame.py\n",
    "     - getDots - Justin Rentie\n",
    "     - runSingleTurn - Wesley Turner\n",
    "     - startGame - Corey Craddock & Wesley Turner\n",
    "     - main - Corey Craddock & Wesley Turner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Waiting for results that Corey will gather."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Public GitHub Repository Used for Development](https://github.com/StaticNomad/PacMan)\n",
    "* [Reinforcement Learning for Two-Player Games](http://nbviewer.jupyter.org/url/www.cs.colostate.edu/~anderson/cs440/notebooks/15%20Reinforcement%20Learning%20for%20Two-Player%20Games.ipynb) \n",
    "* [A5 Reinforcement Learning Solution to the Towers of Hanoi Puzzle](http://nbviewer.jupyter.org/url/www.cs.colostate.edu/~anderson/cs440/notebooks/A5%20Reinforcement%20Learning%20Solution%20to%20Towers%20of%20Hanoi.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
