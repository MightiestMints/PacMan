{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pacman in Automation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authors: Corey Craddock, Braden Hogan, Justin Rentie, Wesley Turner\n",
    "\n",
    "Date Submitted: 12/12/2017\n",
    "\n",
    "Description: Create a game of Pacman with controls to vary levels of Pacman and Ghost intelligence and mazes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our CS 440 Final Project, we chose to implement artificial intelligence algorithms that we learned in the class over the course of this semester to run the classic arcade game Pacman in full automation. We decided this as our final project for a few reasons.\n",
    "\n",
    "First, all four of use have deeply enjoyed learning artificial intelligence using games this semester, and we wanted our last project to reflect this. \n",
    "\n",
    "Second, we have already seen the AI algorithms, which are Iterative Deepening Search and Q Table Reinforcement Learning, implemented within a game setting. We were expecting implementing these algorithms for a new game to be both an obtainable goal, as well as a challenging one.\n",
    "\n",
    "Last, in our initial research for this project, we found that implementing artificial intelligence with Pacman was a common project for computer scientists, taking the form of a rite of a passage for anyone deeply interested in artificial intelligence.\n",
    "\n",
    "Over the course of the last few weeks, we have worked diligently to implement a game of Pacman with these algorithms. Below, we discuss our implementation for the framework of the project, the object used to save the state and players, the Iterative Deepening Search algorithm with Pacman and Ghosts, and the Reinforcement Learning algorithm with Pacman."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've outlined thoughts on design and decisions that we made in implementing each of these sections here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Framework - Corey Craddock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Waiting for english from Corey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gameboard - Braden Hogan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Waiting for english from Braden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative Deepening Search - Justin Rentie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Waiting for english from Justin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning - Wesley Turner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the beginning, we knew we wanted to write a function to train a Q table for the states that Pacman would most likely move through in a game. We planned for this to be our most optimized form of a smart Pacman, as this intelligence made the most sense for our game.\n",
    "\n",
    "We utilized two different sources to implement this. The first was from a previous assignment in this course, Assignment 5. A link for this assignment page can be found below. This assignment had students implement training a Q table and testing it with the Towers of Hanoi puzzle. Although this was a good start, that puzzle is a one-person game, so our reinforcement would need to differ vastly. However, the overall structure and design for the trainQ function initially came from this assignment.\n",
    "\n",
    "Secondly, we took inspiration for the reinforcement for Pacman from the lecture notes titled “Reinforcement Learning for Two-Player Games,” which I have linked below. We knew we needed to augment the reinforcement for different actions that Pacman may take, and the implementation in this lecture was sufficient to start our brainstorming.\n",
    "\n",
    "Despite these sources aiding us with design, we were still running into issues with what actions needed reinforcement. These were the actions we knew we could reinforce:\n",
    "\n",
    " - Change to score\n",
    " - Pacman dies\n",
    " - Pacman wins or loses\n",
    " - Pacman eats a dot\n",
    "\n",
    "We tried a variety of combinations of these reinforcements. For the most part, these were blind tests: which combination of reinforcements would lead Pacman to have the best and fastest results? We ended up landing on these actions for reinforcement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PacMan' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-e1a80c76df10>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mif\u001b[0m \u001b[0mPacMan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mboardMoveTuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcopySelf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmove\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mQ\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mQ\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mPacMan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mboardMoveTuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcopySelf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmove\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m  \u001b[1;31m# initial Q value for new state,move\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mstateNew\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdotsLeft\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdotsLeft\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m#Pacman ate a dot. Medium positive reinforcement\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'PacMan' is not defined"
     ]
    }
   ],
   "source": [
    "if PacMan.boardMoveTuple(copySelf, state, move) not in Q:\n",
    "    Q[PacMan.boardMoveTuple(copySelf, state, move)] = 0  # initial Q value for new state,move\n",
    "\n",
    "if stateNew.dotsLeft < state.dotsLeft:\n",
    "    #Pacman ate a dot. Medium positive reinforcement\n",
    "    Q[PacMan.boardMoveTuple(copySelf, state, move)] += 3\n",
    "else:\n",
    "    #Pacman did not eat a dot. Small negative reinforcement\n",
    "    Q[PacMan.boardMoveTuple(copySelf, state, move)] += -1\n",
    "if dead:\n",
    "    #Pacman lost a life. Large negative reinforcement\n",
    "    Q[PacMan.boardMoveTuple(copySelf, state, move)] = -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This combination of reinforcements were tested to have the best results for Pacman, as it allows him to have some flexibility when choosing a state/move that is not necessarily taking the closest path to a dot (i.e. he needs to run away). We will be displaying the performance that we found in using trainQ and the Q table it creates in a section below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are all methods and files in our project. We've decided to show who authored which section to show that we all worked as evenly as possible in implementing this project.\n",
    " \n",
    " - Dot.py - All methods authored by Justin Rentie\n",
    " - GameBoard.py - All methods authored by Braden Hogan\n",
    " - Ghost.py\n",
    "     - init - Corey Craddock & Justin Rentie\n",
    "     - actions - Corey Craddock & Braden Hogan\n",
    "     - takeAction - Corey Craddock, Braden Hogan, & Justin Rentie\n",
    "     - randomMove - Corey Craddock\n",
    "     - takeActionShortestDistance - Wesley Turner\n",
    "     - depthLimitedSearch - Wesley Turner\n",
    "     - intelligentMove - Wesley Turner\n",
    " - Pacman.py\n",
    "     - init - Corey Craddock & Justin Rentie\n",
    "     - spawnPt - Corey Craddock\n",
    "     - actions - Corey Craddock & Braden Hogan\n",
    "     - takeAction - Corey Craddock, Braden Hogan, & Justin Rentie\n",
    "     - calculateDistance - Justin Rentie\n",
    "     - getClosestDot - Justin Rentie\n",
    "     - directionToObj - Justin Rentie\n",
    "     - fearFactor - Justin Rentie\n",
    "     - depthLimitedSearch - Justin Rentie\n",
    "     - intelligentMove - Justin Rentie\n",
    "     - runFromGhost - Justin Rentie\n",
    "     - makeRandomMove - Justin Rentie\n",
    "     - gameOver - Corey Craddock\n",
    "     - getLives - Corey Craddock\n",
    "     - boardMoveTuple - Wesley Turner\n",
    "     - useReinforcementTable - Wesley Turner\n",
    "     - epsilonGreedy - Wesley Turner\n",
    "     - trainQ - Wesley Turner\n",
    " - PlayGame.py\n",
    "     - getDots - Justin Rentie\n",
    "     - runSingleTurn - Wesley Turner\n",
    "     - startGame - Corey Craddock & Wesley Turner\n",
    "     - main - Corey Craddock & Wesley Turner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Waiting for results that Corey will gather."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement Learning for Two-Player Games: http://nbviewer.jupyter.org/url/www.cs.colostate.edu/~anderson/cs440/notebooks/15%20Reinforcement%20Learning%20for%20Two-Player%20Games.ipynb \n",
    "A5 Reinforcement Learning Solution to the Towers of Hanoi Puzzle: http://nbviewer.jupyter.org/url/www.cs.colostate.edu/~anderson/cs440/notebooks/A5%20Reinforcement%20Learning%20Solution%20to%20Towers%20of%20Hanoi.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
